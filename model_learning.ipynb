{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ysAu28WeZmLp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import shutil\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, EarlyStopping\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D, Permute\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout, Reshape, Multiply, Conv2D, MaxPool2D, LSTM, Add, Lambda, AveragePooling2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D, LeakyReLU, PReLU, GlobalAveragePooling2D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling, RandomTranslation, RandomRotation, RandomZoom\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Nadam, RMSprop\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier \n",
    "from tensorflow.keras.applications import ResNet50, ResNet152\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.applications import EfficientNetB7, EfficientNetB4, EfficientNetB2, EfficientNetB1\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "s7Dhk8WMcFK6",
    "outputId": "7da4f378-c1ed-4320-d658-af4253e49bf2"
   },
   "outputs": [],
   "source": [
    "real_path = 'F:/deepfake_1st/real_MTCNN'\n",
    "fake_path = 'F:/fake_MTCNN_V2' \n",
    "\n",
    "\n",
    "file = []\n",
    "for filename in os.listdir(real_path ):\n",
    "    file.append(real_path + '/' + filename)\n",
    "    \n",
    "real_df = pd.DataFrame(columns = ['file','label'])\n",
    "real_df['file'] = file\n",
    "real_df['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = []\n",
    "for filename in os.listdir(fake_path ):\n",
    "    file.append(fake_path + '/' + filename)\n",
    "    \n",
    "fake_df = pd.DataFrame(columns = ['file','label'])\n",
    "fake_df['file'] = file\n",
    "fake_df['label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    72399\n",
      "0    41475\n",
      "Name: label, dtype: int64\n",
      "1    38985\n",
      "0    22333\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([real_df, fake_df], axis=0)\n",
    "train_df, valid_df = train_test_split(df, test_size=0.35, random_state = 25, stratify = df['label'])\n",
    "\n",
    "print(train_df['label'].value_counts())\n",
    "print(valid_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 113874 validated image filenames.\n",
      "Found 61318 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "image_size = (300,300)\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1/255,)\n",
    "validation_datagen = ImageDataGenerator(rescale=1/255,)\n",
    "\n",
    "train_gen  = train_datagen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    x_col = 'file',\n",
    "    y_col = 'label',\n",
    "    target_size=image_size,    \n",
    "    batch_size=batch_size,                                    \n",
    "    class_mode='raw',\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_gen  = validation_datagen.flow_from_dataframe(\n",
    "    valid_df,\n",
    "    x_col = 'file',\n",
    "    y_col = 'label',\n",
    "    target_size=image_size,    \n",
    "    batch_size=batch_size,                                    \n",
    "    class_mode='raw',\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "xgbi9eOyf8vY",
    "outputId": "3d5d9a98-8118-425a-fa67-663d8f90432e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 300, 3)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "input_shape_2 = (300, 300, 3)\n",
    "output_size = 1\n",
    "\n",
    "print(input_shape_2)\n",
    "print(output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N3qy0vD3WM6D"
   },
   "outputs": [],
   "source": [
    "class RandomRollLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"Shift data\"\"\"\n",
    "\n",
    "    def __init__(self, roll_limit=0.1, u=0.5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.roll_limit = roll_limit\n",
    "        self.u = u\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.non_trainable_weights.append(self.roll_limit)\n",
    "        self.non_trainable_weights.append(self.u)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, pixels, training=None):\n",
    "        if training is None:\n",
    "            training = K.learning_phase()\n",
    "\n",
    "        if not training:\n",
    "            return pixels\n",
    "\n",
    "        if tf.random.uniform(shape=[]) < self.u:\n",
    "            roll_limit = self.roll_limit * pixels.shape[1]\n",
    "            roll_limit = tf.cast(roll_limit, tf.int32)\n",
    "            roll = tf.random.uniform(shape=[], minval=-roll_limit, maxval=roll_limit, dtype=tf.int32)\n",
    "\n",
    "            pixels = tf.roll(pixels, shift=roll, axis=1)\n",
    "\n",
    "        if tf.random.uniform(shape=[]) < self.u:\n",
    "            roll_limit = self.roll_limit * pixels.shape[2]\n",
    "            roll_limit = tf.cast(roll_limit, tf.int32)\n",
    "            roll = tf.random.uniform(shape=[], minval=-roll_limit, maxval=roll_limit, dtype=tf.int32)\n",
    "\n",
    "            pixels = tf.roll(pixels, shift=roll, axis=2)\n",
    "            \n",
    "        return pixels\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'roll_limit': self.roll_limit,\n",
    "            'u': self.u,\n",
    "        }\n",
    "        config.update(super().get_config())\n",
    "\n",
    "        return config\n",
    "\n",
    "class RandomAddLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"Add data\"\"\"\n",
    "\n",
    "    def __init__(self, add_limit=0.1, u=0.5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.add_limit = add_limit\n",
    "        self.u = u\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.non_trainable_weights.append(self.add_limit)\n",
    "        self.non_trainable_weights.append(self.u)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, pixels, training=None):\n",
    "        if training is None:\n",
    "            training = K.learning_phase()\n",
    "\n",
    "        if not training:\n",
    "            return pixels\n",
    "\n",
    "        if tf.random.uniform(shape=[]) < self.u:\n",
    "            add = tf.random.uniform(shape=[], minval=-self.add_limit, maxval=self.add_limit, dtype=tf.float32)\n",
    "            pixels = pixels + add\n",
    "\n",
    "        return pixels\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'add_limit': self.add_limit,\n",
    "            'u': self.u,\n",
    "        }\n",
    "        config.update(super().get_config())\n",
    "\n",
    "        return config\n",
    "\n",
    "class RandomMultipleLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"Multiple data\"\"\"\n",
    "\n",
    "    def __init__(self, multiple_limit=0.5, u=0.5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.multiple_limit = multiple_limit\n",
    "        self.u = u\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.non_trainable_weights.append(self.multiple_limit)\n",
    "        self.non_trainable_weights.append(self.u)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, pixels, training=None):\n",
    "        if training is None:\n",
    "            training = K.learning_phase()\n",
    "\n",
    "        if not training:\n",
    "            return pixels\n",
    "\n",
    "        if tf.random.uniform(shape=[]) < self.u:\n",
    "            multiple = tf.random.uniform(shape=[], minval=-self.multiple_limit, maxval=self.multiple_limit, dtype=tf.float32)\n",
    "            pixels = pixels * (1 + multiple)\n",
    "\n",
    "        return pixels\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'multiple_limit': self.multiple_limit,\n",
    "            'u': self.u,\n",
    "        }\n",
    "        config.update(super().get_config())\n",
    "\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GSnA5KoJeMRr"
   },
   "outputs": [],
   "source": [
    "def thin_resnet_model(input_shape_2, output_size=1, num_clusters=10):\n",
    "    input_2 = Input(shape=input_shape_2)  # pixel\n",
    "\n",
    "    y = input_2\n",
    "    y = Rescaling(scale=1.0/127.5, offset=-1.0)(y)\n",
    "    y = RandomRollLayer(roll_limit=0.2, u=0.8)(y)\n",
    "    y = RandomRotation(factor=(-0.1, 0.1), fill_mode='constant')(y)\n",
    "\n",
    "    # y = RandomTranslation(0.1, 0.1)(y)\n",
    "    # y = RandomRotation(0.1)(y)\n",
    "    # y = RandomZoom(0.1)(y)\n",
    "\n",
    "    # CONV 1\n",
    "    y = Conv2D(64, (7, 7), padding='same')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = MaxPool2D((2, 2), strides=(2, 2))(y)\n",
    "\n",
    "    # CONV 2 - 1\n",
    "    y1 = Conv2D(48, (1, 1), padding='valid')(y)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(48, (3, 3), padding='same')(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(96, (1, 1), padding='valid')(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    \n",
    "    y2 = Conv2D(96, (1, 1), padding='valid')(y)\n",
    "    y2 = BatchNormalization()(y2)\n",
    "    y = Add()([y1, y2])\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    # CONV 2 - 2\n",
    "    y1 = Conv2D(48, (1, 1), padding='valid')(y)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(48, (3, 3), padding='same')(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(96, (1, 1), padding='valid')(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    \n",
    "    y = Add()([y1, y])\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "\n",
    "    # CONV 3 - 1\n",
    "    y1 = Conv2D(96, (1, 1), padding='valid', strides=(2, 2))(y)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(96, (3, 3), padding='same')(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(128, (1, 1), padding='valid')(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "\n",
    "    y2 = AveragePooling2D((2, 2), strides=2, padding='same')(y)\n",
    "    y2 = Conv2D(128, (1, 1), padding='valid')(y2)\n",
    "    # y2 = Conv2D(128, (1, 1), padding='valid', strides=(2, 2))(y)\n",
    "    y2 = BatchNormalization()(y2)\n",
    "    y = Add()([y1, y2])\n",
    "    y = Activation('relu')(y)\n",
    "  \n",
    "    # CONV 3 - 2\n",
    "    y1 = Conv2D(96, (1, 1), padding='valid')(y)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(96, (3, 3), padding='same')(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(128, (1, 1), padding='valid')(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "\n",
    "    y = Add()([y1, y])\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    # CONV 4 - 1\n",
    "    y1 = Conv2D(128, (1, 1), padding='valid', strides=(2, 2))(y)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(128, (3, 3), padding='same')(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(256, (1, 1), padding='valid')(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "\n",
    "    # y2 = Conv2D(256, (1, 1), padding='valid', strides=(2, 2))(y)\n",
    "    y2 = AveragePooling2D((2, 2), strides=2, padding='same')(y)\n",
    "    y2 = Conv2D(256, (1, 1), padding='valid')(y2)\n",
    "    y2 = BatchNormalization()(y2)\n",
    "    y = Add()([y1, y2])\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    # CONV 4 - 2\n",
    "    y1 = Conv2D(128, (1, 1), padding='valid')(y)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(128, (3, 3), padding='same')(y1)\n",
    "    y1 = BatchNormalization(gamma_initializer='zeros')(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(256, (1, 1), padding='valid')(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "\n",
    "    y = Add()([y1, y])\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    # CONV 5 - 1\n",
    "    y1 = Conv2D(256, (1, 1), padding='valid', strides=(2, 2))(y)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(256, (3, 3), padding='same')(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(512, (1, 1), padding='valid')(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "\n",
    "    # y2 = Conv2D(512, (1, 1), padding='valid', strides=(2, 2))(y)\n",
    "    y2 = AveragePooling2D((2, 2), strides=2, padding='same')(y)\n",
    "    y2 = Conv2D(512, (1, 1), padding='valid')(y2)\n",
    "    y2 = BatchNormalization()(y2)\n",
    "    y = Add()([y1, y2])\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    # CONV 5 - 2\n",
    "    y1 = Conv2D(256, (1, 1), padding='valid')(y)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(256, (3, 3), padding='same')(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(512, (1, 1), padding='valid')(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "\n",
    "    y = Add()([y1, y])\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    # CONV 6\n",
    "    y = Conv2D(512, (2, 2), padding='valid')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    # y = Reshape((-1, 512))(y)\n",
    "    # y = Flatten()(y)\n",
    "    y = GlobalAveragePooling2D()(y)\n",
    "    # y = NetVLAD(num_clusters=num_clusters)(y)\n",
    "#     y = tf.keras.layers.Concatenate(axis=1)([y, input_1])\n",
    "    y = Dropout(0.3)(y)\n",
    "\n",
    "    y = Dense(output_size)(y)\n",
    "    y = Activation('sigmoid')(y)\n",
    "    output = y\n",
    "\n",
    "    model = Model(inputs=[input_2], outputs=output, name='thin_resnet_model')\n",
    "    # optimizer = tfa.optimizers.AdamW(learning_rate=0.05, weight_decay=0.0001)\n",
    "    model.compile(loss='mse', optimizer='Nadam', metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cwgwof2-PEeq"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG19\n",
    "\n",
    "\n",
    "def vggnet_model(input_shape_2, output_size=1):\n",
    "    input_2 = Input(shape=input_shape_2)  # pixel\n",
    "\n",
    "    y = input_2\n",
    "\n",
    "#     y = Rescaling(scale=1.0/255.0, offset=-1.0)(y)\n",
    "#     y = tf.keras.layers.experimental.preprocessing.Resizing(height=56, width=56)(y)\n",
    "#     y = RandomRollLayer(roll_limit=0.2, u=0.8)(y)\n",
    "   \n",
    "\n",
    "    model = VGG19(include_top=False, input_tensor=y, pooling='max', input_shape=y.shape[1:], weights=None)\n",
    "\n",
    "    y = model.output\n",
    "    y = Dense(1024)(y)\n",
    "    y = Dropout(0.25)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = Dense(256)(y)\n",
    "    y = Dropout(0.25)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = Dense(output_size, activation='sigmoid', name='softmax')(y)\n",
    "\n",
    "    output = y\n",
    "\n",
    "    model = Model(inputs=[input_2], outputs=output, name='vggnet_model')\n",
    "    model.compile(loss='mse', optimizer='Nadam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oqd8ur_AT4_y"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet101V2\n",
    "\n",
    "\n",
    "def resnet_model(input_shape_2, output_size=1):\n",
    "    input_2 = Input(shape=input_shape_2)  # pixel\n",
    "\n",
    "    y = input_2\n",
    "\n",
    "#     y = Rescaling(scale=1.0/127.5, offset=-1.0)(y)\n",
    "#     y = tf.keras.layers.experimental.preprocessing.Resizing(height=56, width=56)(y)\n",
    "#     y = RandomRollLayer(roll_limit=0.2, u=0.8)(y)\n",
    "   \n",
    "\n",
    "    model = ResNet101V2(include_top=False, input_tensor=y, pooling='max', input_shape=y.shape[1:], weights=None)\n",
    "\n",
    "    y = model.output\n",
    "    y = Dense(1024)(y)\n",
    "    y = Dropout(0.25)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = Dense(256)(y)\n",
    "    y = Dropout(0.25)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = Dense(output_size, activation='sigmoid', name='softmax')(y)\n",
    "\n",
    "    output = y\n",
    "\n",
    "    model = Model(inputs=[input_2], outputs=output, name='resnet_model')\n",
    "    model.compile(loss='mse', optimizer='Nadam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fIwajPzbqrHB"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import DenseNet121\n",
    "\n",
    "\n",
    "def densenet_model(input_shape_2, output_size=1):\n",
    "    input_2 = Input(shape=input_shape_2)  # pixel\n",
    "\n",
    "    y = input_2\n",
    "\n",
    "#     y = Rescaling(scale=1.0/127.5, offset=-1.0)(y)\n",
    "#     y = tf.keras.layers.experimental.preprocessing.Resizing(height=112, width=112)(y)\n",
    "#     y = RandomRollLayer(roll_limit=0.2, u=0.8)(y)\n",
    "#     y = RandomRotation(factor=(-0.1, 0.1), fill_mode='constant')(y)\n",
    "\n",
    "    model = DenseNet121(include_top=False, input_tensor=y, input_shape=y.shape[1:], weights=None, pooling='max')\n",
    "\n",
    "    y = model.output\n",
    "    y = Dense(1024)(y)\n",
    "    y = Dropout(0.25)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    \n",
    "    y = Dense(256)(y)\n",
    "    y = Dropout(0.25)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = Dense(output_size, activation='sigmoid', name='softmax')(y)\n",
    "    output = y\n",
    "\n",
    "    model = Model(inputs=[input_2], outputs=output, name='densenet_model')\n",
    "    model.compile(loss='mse', optimizer='Nadam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nP9RUXuownR_"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import Xception\n",
    "\n",
    "\n",
    "def xception_model(input_shape_2, output_size=1):\n",
    "    input_2 = Input(shape=input_shape_2)  # pixel\n",
    "\n",
    "    y = input_2\n",
    "\n",
    "#     y = Rescaling(scale=1.0/127.5, offset=-1.0)(y)\n",
    "#     y = tf.keras.layers.experimental.preprocessing.Resizing(height=112, width=112)(y)\n",
    "#     y = RandomRollLayer(roll_limit=0.2, u=0.8)(y)\n",
    "#     y = RandomRotation(factor=(-0.1, 0.1), fill_mode='constant')(y)\n",
    "\n",
    "    model = Xception(include_top=False, input_tensor=y, input_shape=y.shape[1:], weights=None, pooling='max')\n",
    "\n",
    "    y = model.output\n",
    "    y = Dense(1024)(y)\n",
    "    y = Dropout(0.25)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = Dense(256)(y)\n",
    "    y = Dropout(0.25)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = Dense(output_size, activation='sigmoid', name='softmax')(y)\n",
    "    output = y\n",
    "\n",
    "    model = Model(inputs=[input_2], outputs=output, name='xception_model')\n",
    "    model.compile(loss='mse', optimizer='Nadam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dHe_a-GIYuZJ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import InceptionV3\n",
    "\n",
    "\n",
    "def inception_model(input_shape_2, output_size=1):\n",
    "    input_2 = Input(shape=input_shape_2)  # pixel\n",
    "\n",
    "    y = input_2\n",
    "\n",
    "#     y = Rescaling(scale=1.0/127.5, offset=-1.0)(y)\n",
    "#     y = tf.keras.layers.experimental.preprocessing.Resizing(height=112, width=112)(y)\n",
    "#     y = RandomRollLayer(roll_limit=0.2, u=0.8)(y)\n",
    "#     y = RandomRotation(factor=(-0.1, 0.1), fill_mode='constant')(y)\n",
    "\n",
    "    model = InceptionV3(include_top=False, input_tensor=y, input_shape=y.shape[1:], weights=None, pooling='max')\n",
    "\n",
    "    y = model.output\n",
    "    y = Dense(1024)(y)\n",
    "    y = Dropout(0.25)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = Dense(256)(y)\n",
    "    y = Dropout(0.25)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = Dense(output_size, activation='sigmoid', name='softmax')(y)\n",
    "    output = y\n",
    "\n",
    "    model = Model(inputs=[input_2], outputs=output, name='inception_model')\n",
    "    model.compile(loss='mse', optimizer='Nadam', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "\n",
    "def MobileNetV2_model(input_shape_2, output_size=1):\n",
    "    input_2 = Input(shape=input_shape_2)  # pixel\n",
    "\n",
    "    y = input_2\n",
    "\n",
    "#     y = Rescaling(scale=1.0/127.5, offset=-1.0)(y)\n",
    "#     y = tf.keras.layers.experimental.preprocessing.Resizing(height=112, width=112)(y)\n",
    "#     y = RandomRollLayer(roll_limit=0.2, u=0.8)(y)\n",
    "#     y = RandomRotation(factor=(-0.1, 0.1), fill_mode='constant')(y)\n",
    "\n",
    "    model = MobileNetV2(include_top=False, input_tensor=y, input_shape=y.shape[1:], weights=None, pooling='max')\n",
    "\n",
    "    y = model.output\n",
    "    y = Dense(1024)(y)\n",
    "    y = Dropout(0.25)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = Dense(256)(y)\n",
    "    y = Dropout(0.25)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = Dense(output_size, activation='sigmoid', name='softmax')(y)\n",
    "    output = y\n",
    "\n",
    "    model = Model(inputs=[input_2], outputs=output, name='inception_model')\n",
    "    model.compile(loss='mse', optimizer='Nadam', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv5_05drop(input_shape_2, output_size=1):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), activation=\"relu\", input_shape=(300, 300 , 3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(Conv2D(128, (3,3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Conv2D(256, (3,3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Conv2D(512, (3,3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    # Fully Connected \n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(optimizer=Nadam(learning_rate=1e-3),\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def C7L2Nadam(input_shape_2, output_size=1):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), activation=\"relu\", input_shape=(300, 300 , 3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(Conv2D(128, (3,3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(256, (3,3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(256, (3,3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(512, (3,3), activation=\"relu\"))\n",
    "    \n",
    "    model.add(Conv2D(512, (3,3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    # Fully Connected \n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(optimizer=Nadam(learning_rate=1e-4),\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def C5L2Nadam_dropout(input_shape_2, output_size=1):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), activation=\"relu\", input_shape=(300, 300 , 3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(128, (3,3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(256, (3,3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(512, (3,3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    # Fully Connected \n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(optimizer=Nadam(learning_rate=1e-4),\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dog_cat_Nadam(input_shape_2, output_size=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(filters=32,\n",
    "                     kernel_size=(3,3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(300,300,3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(filters=64,\n",
    "                     kernel_size=(3,3),\n",
    "                     activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(filters=128,\n",
    "                     kernel_size=(3,3),\n",
    "                     activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(filters=128,\n",
    "                     kernel_size=(3,3),\n",
    "                     activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(units=512,\n",
    "                    activation='relu'))\n",
    "    \n",
    "    model.add(Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer=Nadam(learning_rate=1e-4),\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dog_cat_3C_Nadam(input_shape_2, output_size=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(filters=32,\n",
    "                     kernel_size=(3,3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(300,300,3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(filters=64,\n",
    "                     kernel_size=(3,3),\n",
    "                     activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(filters=128,\n",
    "                     kernel_size=(3,3),\n",
    "                     activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(units=512,\n",
    "                    activation='relu'))\n",
    "    \n",
    "    model.add(Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer=Nadam(learning_rate=1e-4),\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SyC2GYA0H776"
   },
   "outputs": [],
   "source": [
    "# def k_fold_validation(model_fn, n_splits=5, verbose=1):\n",
    "#     kf = KFold(n_splits=n_splits)\n",
    "#     sum_accuracy = 0\n",
    "#     sum_epoch = 0\n",
    "#     start_time = time.time()\n",
    "#     for i, (train_index, val_index) in enumerate(kf.split(train_gen)):\n",
    "#         model = model_fn(input_shape_2=input_shape_2, output_size=output_size)\n",
    "#         early_stopping = EarlyStopping(monitor='val_accuracy', patience=10)\n",
    "\n",
    "# #         train_data = [train_letters[train_index], train_pixels[train_index]]\n",
    "# #         train_label = train_digits[train_index]\n",
    "\n",
    "# #         val_data = [train_letters[val_index], train_pixels[val_index]]\n",
    "# #         val_label = train_digits[val_index]\n",
    "\n",
    "#         history = model.fit(\n",
    "#             train_gen,\n",
    "#             epochs=300,\n",
    "#             validation_data=val_gen,\n",
    "#             batch_size=4,\n",
    "#             verbose=verbose,\n",
    "#             callbacks=[early_stopping],\n",
    "#         )\n",
    "\n",
    "#         sum_epoch += len(history.history['val_accuracy'])\n",
    "#         sum_accuracy += max(history.history['val_accuracy'])\n",
    "        \n",
    "#         if verbose >= 0:            \n",
    "#             print(f'{i+1}/{n_splits} fold result: ')\n",
    "#             print('epoch num:', len(history.history['val_accuracy']))\n",
    "#             print('best val accuracy: ', max(history.history['val_accuracy']))\n",
    "#             print('average 20: ', np.mean(history.history['val_accuracy'][-20:]))\n",
    "#             print('='*50)\n",
    "\n",
    "#     print('Average Accuracy: ', sum_accuracy/n_splits)\n",
    "#     print('Average Epoch: ', sum_epoch/n_splits)\n",
    "#     print('Time taken: ', time.time() - start_time)\n",
    "#     print('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# k_fold_validation(resnet_model, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 실험 결과\n",
    "\n",
    "### Augumentation\n",
    "No Augumentation -> 0.69\n",
    "RandomRollLayer(roll_limit=0.2, u=0.8)(y) -> 0.835\n",
    "Roll + ResNet-D -> 0.824\n",
    "RandomTranslation(height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2), fill_mode='constant')(y) -> 0.768\n",
    "RandomRotation(factor=(-0.2, 0.2), fill_mode='constant')(y) -> 0.731\n",
    "RandomAddLayer(add_limit=0.1, u=0.8)(y) -> 0.751\n",
    "RandomZoom(height_factor=(0, 0.2), width_factor=(0, 0.2), fill_mode='reflect')(y) -> 0.689\n",
    "\n",
    "### Model\n",
    "thin_resnet_model -> 0.835\n",
    "xception_model -> 0.894\n",
    "inception_model -> 0.886\n",
    "densenet_model -> 0.885\n",
    "vggnet_model -> 0.833\n",
    "resnet_model -> 0.840\n",
    "\n",
    "### Loss\n",
    "RMSProp + categorical_crossentropy -> 0.823\n",
    "\n",
    "### Batch size\n",
    "smaller is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zzTmAAqGim7E"
   },
   "outputs": [],
   "source": [
    "model_fn_list = [\n",
    "#     conv5_05drop,\n",
    "#     C7L2Nadam,\n",
    "#     C5L2Nadam_dropout,\n",
    "#     dog_cat_Nadam,\n",
    "    dog_cat_3C_Nadam\n",
    "#     thin_resnet_model,\n",
    "\n",
    "#     vggnet_model,\n",
    "#     MobileNetV2_model,\n",
    "#     resnet_model,\n",
    "#     densenet_model,\n",
    "#     xception_model,\n",
    "#     inception_model\n",
    "]\n",
    "\n",
    "EPOCHS = 300\n",
    "BATCH_SIZE = 20\n",
    "CHECKPOINT_PATH = './checkpoint'\n",
    "MODEL_PATH = './model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mv9vgzQAhymY"
   },
   "outputs": [],
   "source": [
    "if os.path.isdir(CHECKPOINT_PATH):\n",
    "    shutil.rmtree(CHECKPOINT_PATH, ignore_errors=True)\n",
    "os.mkdir(CHECKPOINT_PATH)\n",
    "\n",
    "if os.path.isdir(MODEL_PATH):\n",
    "    shutil.rmtree(MODEL_PATH, ignore_errors=True)\n",
    "os.mkdir(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "colab_type": "code",
    "id": "U3JubR0CjBp1",
    "outputId": "34b357ad-e23b-43e7-cd00-813529f24dd1"
   },
   "outputs": [],
   "source": [
    "for model_fn in model_fn_list:\n",
    "    kf = KFold(n_splits=5)\n",
    "    for i, (train_index, val_index) in enumerate(kf.split(train_gen)):\n",
    "        start_time = time.time()\n",
    "        model = model_fn(input_shape_2=input_shape_2, output_size=output_size)\n",
    "\n",
    "        # Validation 점수가 가장 좋은 모델만 저장합니다.\n",
    "        checkpoint_path = os.path.join(CHECKPOINT_PATH, f'{model.name}_fold{i+1}')\n",
    "        if os.path.isdir(checkpoint_path):\n",
    "            shutil.rmtree(checkpoint_path, ignore_errors=True)\n",
    "        os.mkdir(checkpoint_path)\n",
    "        checkpoint_file_path = os.path.join(checkpoint_path, 'Epoch_{epoch:03d}_Val_{val_loss:.3f}.hdf5')\n",
    "        checkpoint = ModelCheckpoint(filepath=checkpoint_file_path, monitor='val_accuracy', verbose=0, save_best_only=True)\n",
    "\n",
    "        # 30회 간 Validation 점수가 좋아지지 않으면 중지합니다.\n",
    "        early_stopping = EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "\n",
    "#         train_data = [train_letters[train_index], train_pixels[train_index]]\n",
    "#         train_label = train_digits[train_index]\n",
    "\n",
    "#         val_data = [train_letters[val_index], train_pixels[val_index]]\n",
    "#         val_label = train_digits[val_index]\n",
    "\n",
    "        history = model.fit(\n",
    "            train_gen,\n",
    "            epochs=EPOCHS,\n",
    "            validation_data=val_gen,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            verbose=1,\n",
    "            callbacks=[early_stopping, checkpoint],\n",
    "        )\n",
    "\n",
    "        # 가장 좋은 모델의 weight를 불러옵니다.\n",
    "        weigth_file = glob.glob('{}/*.hdf5'.format(checkpoint_path))[-1]\n",
    "        model.load_weights(weigth_file)\n",
    "        model.save(os.path.join(MODEL_PATH, f'{model.name}_{i+1}.h5'))\n",
    "        \n",
    "        shutil.rmtree(checkpoint_path, ignore_errors=True)\n",
    "\n",
    "        epoch_num = len(history.history['val_accuracy'])\n",
    "        max_accuracy = max(history.history['val_accuracy'])\n",
    "        print('='*50)\n",
    "        print(f'Result of {model.name}, fold {i+1}')\n",
    "        print(f'Epoch: {epoch_num}')\n",
    "        print(f'Accuracy: {max_accuracy}')\n",
    "        print('Time taken: ', time.time() - start_time)\n",
    "        print('='*50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('C:/Users/lhj96/OneDrive/notebook_dir/test_ensemble/model/sequential_4100.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4100 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "# generator로 predict 돌리기 (새로운 방식)\n",
    "test_path = 'F:/deepfake_1st/leaderboard_MTCNN'\n",
    "test_data = [f for f in os.listdir(test_path) if f.endswith('.jpg')]\n",
    "test_data = sorted(test_data, key=lambda x: int(x[:-4]))\n",
    "test_datagen = ImageDataGenerator(rescale=1/255,)\n",
    "test_gen  = test_datagen.flow_from_dataframe(\n",
    "    pd.DataFrame({'file':test_data}),\n",
    "    test_path,\n",
    "    x_col='file',\n",
    "    y_col='file',\n",
    "    class_mode = 'raw',\n",
    "    target_size=(300,300),    \n",
    "    batch_size=len(test_data)                                      \n",
    "    )\n",
    "\n",
    "# paths = []\n",
    "# for x, y in test_gen:\n",
    "#     paths = y\n",
    "#     classes = model.predict(x)\n",
    "#     break\n",
    "    \n",
    "# classes = np.where(classes>0.5,1,0)\n",
    "\n",
    "# pred_df = pd.DataFrame({'path':list(map(lambda x: int(x[:-4]),paths)),\n",
    "#                        'y':classes.ravel().astype('int')})\n",
    "# test = pred_df.sort_values('path')\n",
    "# # 제출 파일 제작\n",
    "# sub = pd.read_csv('F:/test/sample_submission.csv')\n",
    "# sub['y'] = test['y'].values\n",
    "# sub.to_csv('F:/test/submission.csv', index=False)\n",
    "# print(sum(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free: 6999MB / 7000MB    0.01% used \n"
     ]
    }
   ],
   "source": [
    "import subprocess as sp\n",
    "import os\n",
    "\n",
    "def get_gpu_memory():\n",
    "    total = 7000\n",
    "    _output_to_list = lambda x: x.decode('ascii').split('\\n')[:-1]\n",
    "\n",
    "    ACCEPTABLE_AVAILABLE_MEMORY = 7000\n",
    "    COMMAND = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "    memory_free_info = _output_to_list(sp.check_output(COMMAND.split()))[1:]\n",
    "    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "    print(f\"Free: {memory_free_values[0]}MB / {total}MB    {(total - memory_free_values[0]) / total * 100:.2f}% used \")\n",
    "    \n",
    "def reset_keras():\n",
    "    sess = tf.compat.v1.keras.backend.get_session()\n",
    "    tf.compat.v1.keras.backend.clear_session()\n",
    "    sess.close()\n",
    "    sess = tf.compat.v1.keras.backend.get_session()\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "    config.gpu_options.visible_device_list = \"0\"\n",
    "    tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))\n",
    "\n",
    "get_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EpyeYgaAIgDO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 sequential_11\n",
      "Free: 2010MB / 7000MB    71.29% used \n",
      "==================================================\n",
      "2 sequential_12\n",
      "Free: 2031MB / 7000MB    70.99% used \n",
      "==================================================\n",
      "3 sequential_14\n",
      "Free: 2029MB / 7000MB    71.01% used \n",
      "==================================================\n",
      "4 sequential_15\n",
      "Free: 2032MB / 7000MB    70.97% used \n",
      "==================================================\n",
      "5 sequential_16\n",
      "Free: 2029MB / 7000MB    71.01% used \n",
      "==================================================\n",
      "6 sequential_1\n",
      "Free: 2037MB / 7000MB    70.90% used \n",
      "==================================================\n",
      "7 sequential_2\n",
      "Free: 2027MB / 7000MB    71.04% used \n",
      "==================================================\n",
      "8 sequential_3\n",
      "Free: 2009MB / 7000MB    71.30% used \n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_PATH = 'F:/test/model'\n",
    "model_files = glob.glob(f'{MODEL_PATH}/*.h5')\n",
    "sum_list = []\n",
    "\n",
    "for i, model_file in enumerate(model_files):\n",
    "    pred_list = []\n",
    "    model = keras.models.load_model(model_file)\n",
    "\n",
    "    print(i+1, model.name)\n",
    "    paths = []\n",
    "    for x, y in test_gen:\n",
    "        paths = y\n",
    "        classes = model.predict(x)\n",
    "        break\n",
    "    \n",
    "    classes = np.where(classes>0.6,1,0)\n",
    "\n",
    "    pred_df = pd.DataFrame({'path':list(map(lambda x: int(x[:-4]),paths)),\n",
    "                           'y':classes.ravel().astype('int')})\n",
    "    test = pred_df.sort_values('path')\n",
    "    # 제출 파일 제작\n",
    "    sub = pd.read_csv('F:/test/sample_submission.csv')\n",
    "    sub['y'] = test['y'].values\n",
    "    sub.to_csv('F:/test/' + str(i) + '_submission.csv', index=False)\n",
    "    \n",
    "    model = None\n",
    "    gc.collect()\n",
    "#     reset_keras()\n",
    "    get_gpu_memory()\n",
    "    print('='*50)\n",
    "display(sum_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 0, ..., 0, 0, 4])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5566"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_list_array=sum_list.copy()\n",
    "# sum_list_array=sum_list_array[0]\n",
    "display(sum_list_array)\n",
    "print(len(sum_list_array[0]))\n",
    "sum(sum_list_array[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "933\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sum_list_array[0])):\n",
    "    if sum_list_array[0][i] <= 3:\n",
    "        sum_list_array[0][i] = 0\n",
    "    else:\n",
    "        sum_list_array[0][i] = 1\n",
    "print(sum(sum_list_array[0]))\n",
    "# pred_df = pd.DataFrame({'path':list(map(lambda x: int(x[:-4]),paths)),\n",
    "#                        'y':sum_list_array.ravel().astype('int')})\n",
    "# test = pred_df.sort_values('path')\n",
    "# 제출 파일 제작\n",
    "sub = pd.read_csv('F:/test/sample_submission.csv')\n",
    "sub['y'] = test['y'].values\n",
    "sub.to_csv('F:/test/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "933\n"
     ]
    }
   ],
   "source": [
    "print(sum(sum_list_array[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>leaderboard/image_00000.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>leaderboard/image_00001.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>leaderboard/image_00002.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>leaderboard/image_00003.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>leaderboard/image_00004.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4095</th>\n",
       "      <td>leaderboard/image_04095.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4096</th>\n",
       "      <td>leaderboard/image_04096.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4097</th>\n",
       "      <td>leaderboard/image_04097.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4098</th>\n",
       "      <td>leaderboard/image_04098.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4099</th>\n",
       "      <td>leaderboard/image_04099.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             path  y\n",
       "0     leaderboard/image_00000.jpg  0\n",
       "1     leaderboard/image_00001.jpg  0\n",
       "2     leaderboard/image_00002.jpg  0\n",
       "3     leaderboard/image_00003.jpg  0\n",
       "4     leaderboard/image_00004.jpg  0\n",
       "...                           ... ..\n",
       "4095  leaderboard/image_04095.jpg  0\n",
       "4096  leaderboard/image_04096.jpg  0\n",
       "4097  leaderboard/image_04097.jpg  0\n",
       "4098  leaderboard/image_04098.jpg  0\n",
       "4099  leaderboard/image_04099.jpg  0\n",
       "\n",
       "[4100 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_data = np.zeros(pred_list[0].shape)\n",
    "\n",
    "for pred in pred_list:\n",
    "    ensemble_data += pred ** 0.5\n",
    "    \n",
    "y_pred = np.argmax(ensemble_data, axis=1)\n",
    "\n",
    "submission_csv = pd.read_csv('F:/test/sample_submission.csv')\n",
    "\n",
    "submission_csv['y'] = y_pred\n",
    "submission_csv.to_csv('F:/test/test.csv', index=False)\n",
    "submission_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ensemble_data)\n",
    "df['max'] = df.iloc[:, 0:10].max(axis=1)\n",
    "df['pred'] = df.iloc[:, 0:10].idxmax(axis=1)\n",
    "\n",
    "good_df = df[df['max'] > 20]\n",
    "\n",
    "test_digits_int = good_df['pred'].to_numpy()\n",
    "test_digits = tf.keras.utils.to_categorical(test_digits_int)\n",
    "\n",
    "new_digits = np.concatenate((train_digits, test_digits))\n",
    "new_letters = np.concatenate((train_letters, test_letters[good_df.index]))\n",
    "new_pixels = np.concatenate((train_pixels, test_pixels[good_df.index]))\n",
    "new_digits.shape, new_letters.shape, new_pixels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_digits = new_digits\n",
    "train_letters = new_letters\n",
    "train_pixels = new_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmKuoQ5ute9G"
   },
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict([test_letters, test_pixels]), axis=1)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rMBz0xqKv9Jo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "submission_csv['digit'] = y_pred\n",
    "submission_csv.to_csv('CNN.csv', index=False)\n",
    "submission_csv\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
