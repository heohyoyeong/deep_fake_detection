{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 제작 및 predict 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델에 필요한 패키지 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리 과정에서 처리한 데이터들을 호출\n",
    "#### 호출후 labeling을 추가하여 DataFrame을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_path = './real_MTCNN/'\n",
    "fake_path = './fake_MTCNN/' \n",
    "\n",
    "\n",
    "file = []\n",
    "for filename in os.listdir(real_path ):\n",
    "    file.append(real_path + '/' + filename)\n",
    "    \n",
    "real_df = pd.DataFrame(columns = ['file','label'])\n",
    "real_df['file'] = file\n",
    "real_df['label'] = 0\n",
    "print(real_df[:3])\n",
    "\n",
    "file = []\n",
    "for filename in os.listdir(fake_path ):\n",
    "    file.append(fake_path + '/' + filename)\n",
    "    \n",
    "fake_df = pd.DataFrame(columns = ['file','label'])\n",
    "fake_df['file'] = file\n",
    "fake_df['label'] = 1\n",
    "print(fake_df[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 저장된 Real DataFrame과 Fake DataFrame을 결하여 기본 DataSet을 구성\n",
    "#### sklearn의 train_test_split 모듈을 활용하여 train DataSet과 valid DataSet으로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([real_df, fake_df], axis=0)\n",
    "train_df, valid_df = train_test_split(df, test_size=0.2, random_state = 25, stratify = df['label'])\n",
    "\n",
    "print(train_df['label'].value_counts())\n",
    "print(valid_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 저장된 데이터셋을 활용하여 ImageDataGenerator를 제작\n",
    "#### 이미지를 model에 입력할수있도록 제작 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "image_size = (300,300)\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1/255,)\n",
    "validation_datagen = ImageDataGenerator(rescale=1/255,)\n",
    "test_datagen = ImageDataGenerator(rescale=1/255,)\n",
    "\n",
    "train_gen  = train_datagen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    x_col = 'file',\n",
    "    y_col = 'label',\n",
    "    target_size=image_size,    \n",
    "    batch_size=batch_size,                                    \n",
    "    class_mode='raw',\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_gen  = validation_datagen.flow_from_dataframe(\n",
    "    valid_df,\n",
    "    x_col = 'file',\n",
    "    y_col = 'label',\n",
    "    target_size=image_size,    \n",
    "    batch_size=batch_size,                                    \n",
    "    class_mode='raw',\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## callbacks 설정 \n",
    "### EarlyStopping\n",
    "#### monitor= 무었을 관찰할것인가 , patience= 몇번이상 줄지않으면 정지할 것 인가\n",
    "### ReduceLROnPlateau\n",
    "#### monitor= 무었을 관찰할것인가, factor= 몇배로 줄일것인가, min_lr= 최저의 학습률은?\n",
    "### ModelCheckpoint\n",
    "#### filepath= 저장하는 위치와 이름, save_best_only= 가장 좋은 모델만을 저장할것인가, save_weights_only= 모델의 가중치만을 저장할것인가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', verbose=2, patience=5)\n",
    "learning_rate_reduction=ReduceLROnPlateau(\n",
    "                        monitor= \"val_loss\", \n",
    "                        patience = 2, \n",
    "                        factor = 0.5, \n",
    "                        min_lr=1e-7,\n",
    "                        verbose=1)\n",
    "\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='./NADAM.h5',\n",
    "                                   monitor='val_loss',\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=False)\n",
    "\n",
    "callbacks = [early_stopping, learning_rate_reduction, checkpointer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 7C2L (Nadam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3,3), activation=\"relu\", input_shape=(300, 300 , 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(256, (3,3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(256, (3,3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(512, (3,3), activation=\"relu\"))\n",
    "\n",
    "model.add(Conv2D(51 2, (3,3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Fully Connected \n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer=Nadam(learning_rate=1e-4),\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy']) \n",
    "\n",
    "model.summary()\n",
    "with tf.device('/device:GPU:0'):\n",
    "    history = model.fit(train_gen,\n",
    "                        steps_per_epoch=len(train_df)//batch_size,\n",
    "                        epochs=10,\n",
    "                        validation_data=val_gen,\n",
    "                        validation_steps=len(valid_df)//batch_size,\n",
    "                        verbose=1,\n",
    "                        callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 5C2L_FullDropout (Nadam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation=\"relu\", input_shape=(300, 300 , 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(256, (3,3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(512, (3,3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Fully Connected \n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer=Nadam(learning_rate=1e-4),\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])  \n",
    "\n",
    "model.summary()\n",
    "with tf.device('/device:GPU:0'):\n",
    "    history = model.fit(train_gen,\n",
    "                        steps_per_epoch=len(train_df)//batch_size,\n",
    "                        epochs=10,\n",
    "                        validation_data=val_gen,\n",
    "                        validation_steps=len(valid_df)//batch_size,\n",
    "                        verbose=1,\n",
    "                        callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 4C1L (Nadam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Conv2D(filters=32,\n",
    "                 kernel_size=(3,3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(300,300,3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=64,\n",
    "                 kernel_size=(3,3),\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=128,\n",
    "                 kernel_size=(3,3),\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=128,\n",
    "                 kernel_size=(3,3),\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(units=512,\n",
    "                activation='relu'))\n",
    "\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=Nadam(learning_rate=1e-4),\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "with tf.device('/device:GPU:0'):\n",
    "    history = model.fit(train_gen,\n",
    "                        steps_per_epoch=len(train_df)//batch_size,\n",
    "                        epochs=10,\n",
    "                        validation_data=val_gen,\n",
    "                        validation_steps=len(valid_df)//batch_size,\n",
    "                        verbose=1,\n",
    "                        callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 교육시킨 모델에 대한 성능을 plot으로 확인\n",
    "#### 교육을 진행할때 verbose를 1로 설정하여 획득한 accuracy와 val_accuracy, loss와 val_loss를 \n",
    "#### 진행되는 epoch에 따라 그래프 생성하여 모델의 성능을 그림으로 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.plot(train_acc, 'bo', color='r', label='training accuracy')\n",
    "plt.plot(val_acc, 'b', color='b', label='validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_loss, 'bo', color='r', label='training loss')\n",
    "plt.plot(val_loss, 'b', color='b', label='validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 교육시킨 모델을 활용하여 실제 데이터를 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = './leaderboard_MTCNN'\n",
    "test_data = [f for f in os.listdir(test_path) if f.endswith('.jpg')]\n",
    "test_data = sorted(test_data, key=lambda x: int(x[:-4]))\n",
    "test_datagen = ImageDataGenerator(rescale=1./255,)\n",
    "\n",
    "test_gen  = test_datagen.flow_from_dataframe(\n",
    "    pd.DataFrame({'file':test_data}),\n",
    "    test_path,\n",
    "    x_col='file',\n",
    "    y_col='file',\n",
    "    class_mode = 'raw',\n",
    "    target_size=(300,300),    \n",
    "    batch_size=len(test_data),\n",
    "    shuffle=False\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "paths = []\n",
    "for x, y in test_gen:\n",
    "    paths = y\n",
    "    classes = model.predict(x)\n",
    "    break\n",
    "    \n",
    "for i in range(len(classes)):\n",
    "    if classes[i] < 0.5:\n",
    "        classes[i] = 0\n",
    "    else:\n",
    "        classes[i] = 1\n",
    "pred_df = pd.DataFrame({'path':list(map(lambda x: int(x[:-4]),paths)),\n",
    "                       'y':classes.ravel().astype('int')})\n",
    "test = pred_df.sort_values('path')\n",
    "# 제출 파일 제작\n",
    "sub = pd.read_csv('sample_submission.csv')\n",
    "sub['y'] = test['y'].values\n",
    "sub.to_csv('NADAM.csv', index=False)\n",
    "print(sum(classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_env_tensorflow2] *",
   "language": "python",
   "name": "conda-env-data_env_tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
